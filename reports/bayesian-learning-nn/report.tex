\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=2.35cm, right=3.35cm, top=3.35cm, bottom=3.0cm]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[font={small,it}]{caption}
\graphicspath{ {figures/} }
\usepackage{url}
\usepackage{appendix}
\usepackage{float}
\usepackage[bottom]{footmisc}
\usepackage{titling}
\setlength{\droptitle}{-10em}  

\title{ \huge Artificial neural networks \\ 
  { \large Assignment 2: Bayesian learning in neural networks }}
\author{
        Lood, CÃ©dric \\
        \small Master of Bioinformatics
}

\begin{document}
\maketitle
%\tableofcontents

\section{Context}
Bayesian statistics offer a robust framework in which it is possible
to build and assess models, and perform inference.

In this exercise, we were asked to explore bayesian learning applied
to neural networks. Using the Bayes rule in the context of learning
has several desirable properties related to the fact that one is not
using point estimates in the search space of solutions (for example
the weight space) but tries to obtain a distribution over the weight
space. Hence, quantification of certainty of predictions is possible
without the use of validation set or resampling.

\section{Simple perceptron}

\bibliographystyle{ieeetr} 
\bibliography{bib-db}

\end{document}
